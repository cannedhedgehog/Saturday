{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZIN9prDoo9QWtain+xjXi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cannedhedgehog/Saturday/blob/main/Saharnova_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "RmEqIaSYJzh2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем необходимые ресурсы NLTK"
      ],
      "metadata": {
        "id": "eurwqkYVOcrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPf2_TvYObAb",
        "outputId": "aa75aefe-3c6f-45d0-fa3e-6a310cdbb0de"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\", force=True)\n",
        "nltk.download(\"stopwords\", force=True)\n",
        "nltk.download(\"wordnet\", force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Mswg2yRp-v",
        "outputId": "904d5b2b-8d67-4117-b146-0732ce3ea167"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Очистка текста: приведение к нижнему регистру, удаление знаков препинания"
      ],
      "metadata": {
        "id": "6WsUC-_PPYyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Удаляем знаки препинания\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "p0k3y5j5Os5x"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация, удаление стоп-слов и лемматизация"
      ],
      "metadata": {
        "id": "K-4Yw4uuPe_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_lemmatize(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Токенизация, удаление стоп-слов и лемматизация.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    return words"
      ],
      "metadata": {
        "id": "5G5yO1x7PeUw"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация Bag of Words"
      ],
      "metadata": {
        "id": "wPzLzrVEPq2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(corpus: list[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Реализация Bag of Words.\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    print(\"\\nBag of Words (BoW) Matrix:\")\n",
        "    print(bow_matrix.toarray())\n",
        "    print(\"\\nFeature Names:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "    return bow_matrix\n"
      ],
      "metadata": {
        "id": "Yb9oi4LwPro_"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация TF-IDF"
      ],
      "metadata": {
        "id": "e_GfO7UmPyOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf(corpus: list[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Реализация TF-IDF.\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "    print(\"\\nTF-IDF Matrix:\")\n",
        "    print(tfidf_matrix.toarray())\n",
        "    print(\"\\nFeature Names:\", vectorizer.get_feature_names_out())\n",
        "\n",
        "    return tfidf_matrix"
      ],
      "metadata": {
        "id": "tekbae2qP1dn"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_tokenizer(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Токенизация всех ASCII-символов.\n",
        "    \"\"\"\n",
        "    return [char for char in text if char in string.printable]\n"
      ],
      "metadata": {
        "id": "aTBT7hFITR8m"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_vectorizer(text: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Векторизация ASCII-символов.\n",
        "    \"\"\"\n",
        "    return [ord(char) for char in text if char in string.printable]\n"
      ],
      "metadata": {
        "id": "hZd7AsK1TSku"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens: list[str]) -> list[int]:\n",
        "    \"\"\"\n",
        "    Векторизация текста через индексацию уникальных токенов.\n",
        "    \"\"\"\n",
        "    dict_vectors = {}\n",
        "    result = []\n",
        "    for word in tokens:\n",
        "        if word in dict_vectors.keys():\n",
        "            result.append(dict_vectors[word])\n",
        "        else:\n",
        "            dict_vectors[word] = len(dict_vectors)\n",
        "            result.append(dict_vectors[word])\n",
        "    return result"
      ],
      "metadata": {
        "id": "rgLeSFeETUcW"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Основная функция программы.\n",
        "    \"\"\"\n",
        "    documents = [\n",
        "        \"Beneath the sky so vast and bright,\",\n",
        "        \"A silver river sings at night.\",\n",
        "        \"The whispering leaves in twilight dance,\",\n",
        "        \"Spinning dreams in moonlight’s glance.\",\n",
        "        \"A distant echo, soft and free,\",\n",
        "        \"Drifts across the endless sea.\",\n",
        "        \"The stars ignite, then fade away—\",\n",
        "        \"A fleeting spark, a breath, a day.\"\n",
        "    ]\n",
        "\n",
        "    preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
        "    tokenized_docs = [\" \".join(tokenize_and_lemmatize(doc)) for doc in preprocessed_docs]\n",
        "\n",
        "    print(\"\\nPreprocessed Documents:\", tokenized_docs)\n",
        "\n",
        "    bow_matrix = bag_of_words(tokenized_docs)\n",
        "    tfidf_matrix = tf_idf(tokenized_docs)\n",
        "\n",
        "    print(\"\\nASCII Tokenization:\", ascii_tokenizer(documents[0]))\n",
        "    print(\"\\nASCII Vectorization:\", ascii_vectorizer(documents[0]))\n",
        "\n",
        "    print(\"\\nVectorized Tokens (Lemmatized):\", vectorize(tokenize_and_lemmatize(documents[0])))\n",
        "    print(\"\\nVectorized Tokens (Preprocessed):\", vectorize(preprocessed_docs[0].split()))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGe_oWupP9SP",
        "outputId": "70f04455-89d6-4eb9-e562-7f349a160345"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessed Documents: ['beneath sky vast bright', 'silver river sings night', 'whispering leaf twilight dance', 'spinning dream moonlight glance', 'distant echo soft free', 'drift across endless sea', 'star ignite fade away', 'fleeting spark breath day']\n",
            "\n",
            "Bag of Words (BoW) Matrix:\n",
            "[[0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
            "\n",
            "Feature Names: ['across' 'away' 'beneath' 'breath' 'bright' 'dance' 'day' 'distant'\n",
            " 'dream' 'drift' 'echo' 'endless' 'fade' 'fleeting' 'free' 'glance'\n",
            " 'ignite' 'leaf' 'moonlight' 'night' 'river' 'sea' 'silver' 'sings' 'sky'\n",
            " 'soft' 'spark' 'spinning' 'star' 'twilight' 'vast' 'whispering']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.  0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.5 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.5 0.5 0.  0.5 0.5 0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.5 0.  0.\n",
            "  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.5 0.  0.  0.  0.5 0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0. ]\n",
            " [0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.5 0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.5 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.5 0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0. ]\n",
            " [0.  0.  0.  0.5 0.  0.  0.5 0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.5 0.  0.  0.  0.  0. ]]\n",
            "\n",
            "Feature Names: ['across' 'away' 'beneath' 'breath' 'bright' 'dance' 'day' 'distant'\n",
            " 'dream' 'drift' 'echo' 'endless' 'fade' 'fleeting' 'free' 'glance'\n",
            " 'ignite' 'leaf' 'moonlight' 'night' 'river' 'sea' 'silver' 'sings' 'sky'\n",
            " 'soft' 'spark' 'spinning' 'star' 'twilight' 'vast' 'whispering']\n",
            "\n",
            "ASCII Tokenization: ['B', 'e', 'n', 'e', 'a', 't', 'h', ' ', 't', 'h', 'e', ' ', 's', 'k', 'y', ' ', 's', 'o', ' ', 'v', 'a', 's', 't', ' ', 'a', 'n', 'd', ' ', 'b', 'r', 'i', 'g', 'h', 't', ',']\n",
            "\n",
            "ASCII Vectorization: [66, 101, 110, 101, 97, 116, 104, 32, 116, 104, 101, 32, 115, 107, 121, 32, 115, 111, 32, 118, 97, 115, 116, 32, 97, 110, 100, 32, 98, 114, 105, 103, 104, 116, 44]\n",
            "\n",
            "Vectorized Tokens (Lemmatized): [0, 1, 2, 3, 4]\n",
            "\n",
            "Vectorized Tokens (Preprocessed): [0, 1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ]
    }
  ]
}