{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi7vhfVIGZoMe/U4nVicMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cannedhedgehog/Saturday/blob/main/%D0%9B%D0%B0%D0%B1%D0%B0_1_%D1%81%D1%83%D0%B1%D0%B1%D0%BE%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhq9jqCxR2ng",
        "outputId": "8d9308ee-8d68-4ca7-b862-a7647cd2f136"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем библиотеки"
      ],
      "metadata": {
        "id": "zFzpVJWRS_5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pymorphy3\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "metadata": {
        "id": "rL54imCiS-iv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем необходимые пакеты"
      ],
      "metadata": {
        "id": "hoAuJxLVTDhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8x6FlYHTHGm",
        "outputId": "1d26fb78-788a-4b32-c2f8-bc92d252341a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализация MorphAnalyzer для лемматизации"
      ],
      "metadata": {
        "id": "6WZJQkrJTH2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "morph3 = pymorphy3.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "HfEfbmIuTKdO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для лемматизации текста"
      ],
      "metadata": {
        "id": "ViRLa87kTMSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematize(text: str) -> list[str]:\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_words = [morph3.parse(word)[0].normal_form for word in tokens]\n",
        "    return lemmatized_words"
      ],
      "metadata": {
        "id": "yrq6uHHsTOZG"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для стемминга текста"
      ],
      "metadata": {
        "id": "-VO0KAEeTQPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text: str) -> list[str]:\n",
        "    stemmer = SnowballStemmer(\"russian\")\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_words"
      ],
      "metadata": {
        "id": "M6jcQQoqTVbO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для токенизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "Pohjyx7JTZGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_tokenizer(text: str) -> list[str]:\n",
        "    return [char for char in text if char in string.printable]"
      ],
      "metadata": {
        "id": "5hts90XPTbDW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для векторизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "zzJurIcRTdAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_vectorizer(text: str) -> list[int]:\n",
        "    return [ord(char) for char in text if char in string.printable]"
      ],
      "metadata": {
        "id": "BE0M66r_Tf0W"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для токенизации текста"
      ],
      "metadata": {
        "id": "PIA90ibdTj3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text: str) -> list[str]:\n",
        "    return word_tokenize(text)"
      ],
      "metadata": {
        "id": "d74UiW-8TlF-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для векторизации текста"
      ],
      "metadata": {
        "id": "nUyfqenITmnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens: list[str]) -> list[int]:\n",
        "    dict_vectors = {}\n",
        "    result = []\n",
        "    for word in tokens:\n",
        "        if word in dict_vectors:\n",
        "            result.append(dict_vectors[word])\n",
        "        else:\n",
        "            dict_vectors[word] = len(dict_vectors)\n",
        "            result.append(dict_vectors[word])\n",
        "    return result"
      ],
      "metadata": {
        "id": "SyJ4flqKTron"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текст"
      ],
      "metadata": {
        "id": "EapG70gRTuFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Основной код\n",
        "text = 'Летний дождь нежно стучит по окнам, а в воздухе витает запах свежей травы. На улице мирно пасутся овцы, и где-то вдали слышны голоса детей. Солнце едва виднеется сквозь облака, и природа кажется особенно живой. Ветер ласково колышет деревья, создавая звуки, похожие на тихую мелодию.'\n",
        "english_text = 'The summer rain gently taps on the windows, and the air is filled with the scent of fresh grass. Sheep peacefully graze outside, and in the distance, the voices of children can be heard. The sun barely peeks through the clouds, and nature seems especially alive. The wind gently rustles the trees, creating sounds that resemble a quiet melody.'"
      ],
      "metadata": {
        "id": "9oHnMIPYUM0x"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Лемматизация\n",
        "print('Лемматизация:')\n",
        "print(lematize(text))\n",
        "\n",
        "# Стемминг\n",
        "print('Стемминг:')\n",
        "print(stemming(text))\n",
        "\n",
        "# Токенизация всех символов из ASCII\n",
        "print('Токенизация всех символов из ASCII:')\n",
        "print(ascii_tokenizer(english_text))\n",
        "\n",
        "# Векторизация всех символов из ASCII\n",
        "print('Векторизация всех символов из ASCII:')\n",
        "print(ascii_vectorizer(english_text))\n",
        "\n",
        "# Векторизация текста после лемматизации\n",
        "print('Векторизация текста после лемматизации:')\n",
        "print(vectorize(lematize(text)))\n",
        "\n",
        "# Векторизация текста после стемминга\n",
        "print('Векторизация текста после стемминга:')\n",
        "print(vectorize(stemming(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SXPf7qEUVAe",
        "outputId": "5c8e7fd6-2d09-4767-e765-0502fe870acf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизация:\n",
            "['летний', 'дождь', 'нежно', 'стучать', 'по', 'окно', ',', 'а', 'в', 'воздух', 'витать', 'запах', 'свежеть', 'трава', '.', 'на', 'улица', 'мирно', 'пастись', 'овца', ',', 'и', 'где-то', 'вдали', 'слышный', 'голос', 'ребёнок', '.', 'солнце', 'едва', 'виднеться', 'сквозь', 'облако', ',', 'и', 'природа', 'казаться', 'особенно', 'живой', '.', 'ветер', 'ласково', 'колыхать', 'дерево', ',', 'создавать', 'звук', ',', 'похожий', 'на', 'тихий', 'мелодия', '.']\n",
            "Стемминг:\n",
            "['летн', 'дожд', 'нежн', 'стуч', 'по', 'окн', ',', 'а', 'в', 'воздух', 'вита', 'зап', 'свеж', 'трав', '.', 'на', 'улиц', 'мирн', 'пасут', 'овц', ',', 'и', 'где-т', 'вдал', 'слышн', 'голос', 'дет', '.', 'солнц', 'едв', 'виднеет', 'сквоз', 'облак', ',', 'и', 'природ', 'кажет', 'особен', 'жив', '.', 'ветер', 'ласков', 'колышет', 'дерев', ',', 'создав', 'звук', ',', 'похож', 'на', 'тих', 'мелод', '.']\n",
            "Токенизация всех символов из ASCII:\n",
            "['T', 'h', 'e', ' ', 's', 'u', 'm', 'm', 'e', 'r', ' ', 'r', 'a', 'i', 'n', ' ', 'g', 'e', 'n', 't', 'l', 'y', ' ', 't', 'a', 'p', 's', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'w', 'i', 'n', 'd', 'o', 'w', 's', ',', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'a', 'i', 'r', ' ', 'i', 's', ' ', 'f', 'i', 'l', 'l', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 's', 'c', 'e', 'n', 't', ' ', 'o', 'f', ' ', 'f', 'r', 'e', 's', 'h', ' ', 'g', 'r', 'a', 's', 's', '.', ' ', 'S', 'h', 'e', 'e', 'p', ' ', 'p', 'e', 'a', 'c', 'e', 'f', 'u', 'l', 'l', 'y', ' ', 'g', 'r', 'a', 'z', 'e', ' ', 'o', 'u', 't', 's', 'i', 'd', 'e', ',', ' ', 'a', 'n', 'd', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'i', 's', 't', 'a', 'n', 'c', 'e', ',', ' ', 't', 'h', 'e', ' ', 'v', 'o', 'i', 'c', 'e', 's', ' ', 'o', 'f', ' ', 'c', 'h', 'i', 'l', 'd', 'r', 'e', 'n', ' ', 'c', 'a', 'n', ' ', 'b', 'e', ' ', 'h', 'e', 'a', 'r', 'd', '.', ' ', 'T', 'h', 'e', ' ', 's', 'u', 'n', ' ', 'b', 'a', 'r', 'e', 'l', 'y', ' ', 'p', 'e', 'e', 'k', 's', ' ', 't', 'h', 'r', 'o', 'u', 'g', 'h', ' ', 't', 'h', 'e', ' ', 'c', 'l', 'o', 'u', 'd', 's', ',', ' ', 'a', 'n', 'd', ' ', 'n', 'a', 't', 'u', 'r', 'e', ' ', 's', 'e', 'e', 'm', 's', ' ', 'e', 's', 'p', 'e', 'c', 'i', 'a', 'l', 'l', 'y', ' ', 'a', 'l', 'i', 'v', 'e', '.', ' ', 'T', 'h', 'e', ' ', 'w', 'i', 'n', 'd', ' ', 'g', 'e', 'n', 't', 'l', 'y', ' ', 'r', 'u', 's', 't', 'l', 'e', 's', ' ', 't', 'h', 'e', ' ', 't', 'r', 'e', 'e', 's', ',', ' ', 'c', 'r', 'e', 'a', 't', 'i', 'n', 'g', ' ', 's', 'o', 'u', 'n', 'd', 's', ' ', 't', 'h', 'a', 't', ' ', 'r', 'e', 's', 'e', 'm', 'b', 'l', 'e', ' ', 'a', ' ', 'q', 'u', 'i', 'e', 't', ' ', 'm', 'e', 'l', 'o', 'd', 'y', '.']\n",
            "Векторизация всех символов из ASCII:\n",
            "[84, 104, 101, 32, 115, 117, 109, 109, 101, 114, 32, 114, 97, 105, 110, 32, 103, 101, 110, 116, 108, 121, 32, 116, 97, 112, 115, 32, 111, 110, 32, 116, 104, 101, 32, 119, 105, 110, 100, 111, 119, 115, 44, 32, 97, 110, 100, 32, 116, 104, 101, 32, 97, 105, 114, 32, 105, 115, 32, 102, 105, 108, 108, 101, 100, 32, 119, 105, 116, 104, 32, 116, 104, 101, 32, 115, 99, 101, 110, 116, 32, 111, 102, 32, 102, 114, 101, 115, 104, 32, 103, 114, 97, 115, 115, 46, 32, 83, 104, 101, 101, 112, 32, 112, 101, 97, 99, 101, 102, 117, 108, 108, 121, 32, 103, 114, 97, 122, 101, 32, 111, 117, 116, 115, 105, 100, 101, 44, 32, 97, 110, 100, 32, 105, 110, 32, 116, 104, 101, 32, 100, 105, 115, 116, 97, 110, 99, 101, 44, 32, 116, 104, 101, 32, 118, 111, 105, 99, 101, 115, 32, 111, 102, 32, 99, 104, 105, 108, 100, 114, 101, 110, 32, 99, 97, 110, 32, 98, 101, 32, 104, 101, 97, 114, 100, 46, 32, 84, 104, 101, 32, 115, 117, 110, 32, 98, 97, 114, 101, 108, 121, 32, 112, 101, 101, 107, 115, 32, 116, 104, 114, 111, 117, 103, 104, 32, 116, 104, 101, 32, 99, 108, 111, 117, 100, 115, 44, 32, 97, 110, 100, 32, 110, 97, 116, 117, 114, 101, 32, 115, 101, 101, 109, 115, 32, 101, 115, 112, 101, 99, 105, 97, 108, 108, 121, 32, 97, 108, 105, 118, 101, 46, 32, 84, 104, 101, 32, 119, 105, 110, 100, 32, 103, 101, 110, 116, 108, 121, 32, 114, 117, 115, 116, 108, 101, 115, 32, 116, 104, 101, 32, 116, 114, 101, 101, 115, 44, 32, 99, 114, 101, 97, 116, 105, 110, 103, 32, 115, 111, 117, 110, 100, 115, 32, 116, 104, 97, 116, 32, 114, 101, 115, 101, 109, 98, 108, 101, 32, 97, 32, 113, 117, 105, 101, 116, 32, 109, 101, 108, 111, 100, 121, 46]\n",
            "Векторизация текста после лемматизации:\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 6, 20, 21, 22, 23, 24, 25, 14, 26, 27, 28, 29, 30, 6, 20, 31, 32, 33, 34, 14, 35, 36, 37, 38, 6, 39, 40, 6, 41, 15, 42, 43, 14]\n",
            "Векторизация текста после стемминга:\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 6, 20, 21, 22, 23, 24, 25, 14, 26, 27, 28, 29, 30, 6, 20, 31, 32, 33, 34, 14, 35, 36, 37, 38, 6, 39, 40, 6, 41, 15, 42, 43, 14]\n"
          ]
        }
      ]
    }
  ]
}